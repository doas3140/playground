{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torchtext import data\n",
    "import spacy\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['iron cement is a ready for use paste which is laid as a fillet by putty knife or finger in the mould edges ( corners ) of the steel ingot mould .', 'iron cement protects the ingot against the hot , abrasive steel casting process .', 'a fire restant repair cement for fire places , ovens , open fireplaces etc .', 'Construction and repair of highways and ...', 'An announcement must be commercial character .']\n",
      "['iron cement ist eine gebrauchs ##AT##-##AT## fertige Paste , die mit einem Spachtel oder den Fingern als Hohlkehle in die Formecken ( Winkel ) der Stahlguss -Kokille aufgetragen wird .', 'Nach der Aushärtung schützt iron cement die Kokille gegen den heissen , abrasiven Stahlguss .', 'feuerfester Reparaturkitt für Feuerungsanlagen , Öfen , offene Feuerstellen etc.', 'Der Bau und die Reparatur der Autostraßen ...', 'die Mitteilungen sollen den geschäftlichen kommerziellen Charakter tragen .']\n"
     ]
    }
   ],
   "source": [
    "en_text = open('./data/EN_DE_translation/en.txt', encoding=\"utf8\").read().split('\\n')\n",
    "de_text = open('./data/EN_DE_translation/de.txt', encoding=\"utf8\").read().split('\\n')\n",
    "print(en_text[:5])\n",
    "print(de_text[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'happy', '!'] ['i', \"'m\", 'happy', '...']\n",
      "iron cement ist eine gebrauchs ##AT##-##AT## fertige Paste , die mit einem Spachtel oder den Fingern als Hohlkehle in die Formecken ( Winkel ) der Stahlguss -Kokille aufgetragen wird .\n"
     ]
    }
   ],
   "source": [
    "def create_tokenize_fn(tokenizer):\n",
    "    return lambda sentence: [word.text for word in tokenizer(sentence.lower())]\n",
    "\n",
    "disable = [\"tagger\",\"parser\",\"ner\",\"textcat\"] # disable pipelines other then tokenization\n",
    "tokenize_en = create_tokenize_fn(tokenizer=spacy.load('en_core_web_sm', disable=disable))\n",
    "tokenize_de = create_tokenize_fn(tokenizer=spacy.load('de_core_news_sm', disable=disable))\n",
    "\n",
    "print(tokenize_en(\"I am happy!\"), tokenize_en(\"I'm happy...\"))\n",
    "print(de_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_field = data.Field(lower=True, tokenize=tokenize_en)\n",
    "de_field = data.Field(lower=True, tokenize=tokenize_de, init_token='<sos>', eos_token='<eos>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68.3418972492218\n",
      "['en'] ['de']\n"
     ]
    }
   ],
   "source": [
    "raw_data = {'en': [sen for sen in en_text], 'de': [sen for sen in de_text]}\n",
    "df = pd.DataFrame(raw_data)\n",
    "temp_csv = './data/EN_DE_translation/temp.csv'\n",
    "df.to_csv(temp_csv, index=False)\n",
    "t = time()\n",
    "dataset = data.TabularDataset(temp_csv, format='csv', fields=[('x',en_field),('y',de_field)])\n",
    "print(time()-t)\n",
    "print(dataset.examples[0].x, dataset.examples[0].y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_field.build_vocab(dataset)\n",
    "de_field.build_vocab(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n"
     ]
    }
   ],
   "source": [
    "en_pad_nr = en_field.vocab.stoi['<pad>'] # dict word(str) -> unique index(nr)\n",
    "de_pad_nr = de_field.vocab.stoi['<pad>']\n",
    "print(en_pad_nr, de_pad_nr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchSizeFnCreator():\n",
    "    def __init__(self):\n",
    "        self.y_max, self.x_max = 0, 0 # max x/y seq len\n",
    "        \n",
    "    def batch_size_fn(self, new, count, sofar):\n",
    "        self.x_max = max(self.x_max,  len(new.x))\n",
    "        self.y_max = max(self.y_max,  len(new.y) + 2)\n",
    "        x_elements = count * self.x_max\n",
    "        y_elements = count * self.y_max\n",
    "        return max(x_elements, y_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyIterator(data.Iterator):    \n",
    "    def pool(self):\n",
    "        for p in data.batch(self.data(), self.batch_size * 100):\n",
    "            p_batch = data.batch(sorted(p, key=self.sort_key), self.batch_size, self.batch_size_fn)\n",
    "            for b in self.random_shuffler(list(p_batch)):\n",
    "                yield b\n",
    "    \n",
    "    def create_batches(self):\n",
    "        if self.train:\n",
    "            self.batches = self.pool()\n",
    "        else:\n",
    "            self.batches = []\n",
    "            for b in data.batch(self.data(), self.batch_size, self.batch_size_fn):\n",
    "                self.batches.append(sorted(b, key=self.sort_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = BatchSizeFnCreator()\n",
    "iterator = MyIterator(  dataset, batch_size=1500, device=torch.device('cuda'), repeat=False, \n",
    "                        sort_key=lambda e: [len(e.x), len(e.y)],\n",
    "                        batch_size_fn=B.batch_size_fn, train=True, shuffle=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3929"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([i for i,b in enumerate(iterator)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 49],\n",
      "        [271],\n",
      "        [  8],\n",
      "        [308]]) \n",
      " tensor([[ 49],\n",
      "        [271],\n",
      "        [  8],\n",
      "        [308]]) \n",
      " both are the same\n"
     ]
    }
   ],
   "source": [
    "a = en_field.tokenize('I want to go')\n",
    "b = en_field.numericalize([a])\n",
    "c = en_field.process([['i','want','to','go']])\n",
    "print(b,'\\n', c, '\\n both are the same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', '<pad>', 'the', ',', '.', '#', 'and', 'of', 'to', 'a']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_field.vocab.stoi # str -> int\n",
    "en_field.vocab.itos[:10] # int -> str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 0, 0, 0],\n",
       "         [1, 1, 0, 0],\n",
       "         [1, 1, 1, 0],\n",
       "         [1, 1, 1, 1]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 4\n",
    "~torch.triu(torch.ones(1,s,s), diagonal=1).byte()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_enc_dec_masks(x, y, x_pad=en_pad_nr, y_pad=de_pad_nr):\n",
    "    # encoder input mask and decoder input mask\n",
    "    e_x_mask = (x != x_pad).unsqueeze(-2) # [b,1,s(en)]\n",
    "    d_x_mask = (y != y_pad).unsqueeze(-2) # [b,1,s(de)]\n",
    "    s = d_x_mask.size(2)\n",
    "    nopeak_mask = ~torch.triu(torch.ones(1,s,s), diagonal=1).byte().cuda()\n",
    "    # [b,s,s] = [b,1,s] & [b,s,s]\n",
    "    d_x_mask = d_x_mask & nopeak_mask\n",
    "    # return [b,1,s], [b,s,s]\n",
    "    return e_x_mask, d_x_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 28]) torch.Size([10, 28]) torch.Size([10, 1, 28]) torch.Size([10, 28, 28])\n",
      "torch.Size([10, 14]) torch.Size([10, 14]) torch.Size([10, 1, 14]) torch.Size([10, 14, 14])\n",
      "torch.Size([10, 42]) torch.Size([10, 39]) torch.Size([10, 1, 42]) torch.Size([10, 39, 39])\n",
      "torch.Size([10, 33]) torch.Size([10, 35]) torch.Size([10, 1, 33]) torch.Size([10, 35, 35])\n",
      "torch.Size([10, 8]) torch.Size([10, 10]) torch.Size([10, 1, 8]) torch.Size([10, 10, 10])\n",
      "torch.Size([10, 79]) torch.Size([10, 32]) torch.Size([10, 1, 79]) torch.Size([10, 32, 32])\n",
      "torch.Size([10, 48]) torch.Size([10, 23]) torch.Size([10, 1, 48]) torch.Size([10, 23, 23])\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(iterator):\n",
    "    x = batch.x.transpose(0,1)\n",
    "    y = batch.y.transpose(0,1)\n",
    "    e_x_mask, d_x_mask = create_enc_dec_masks(x,y)\n",
    "    # [b,e_s], [b,d_s], [b,1,e_s], [b,d_s,d_s], where e_s = num of words in encoder sentence, b = batch size\n",
    "    print(x.shape, y.shape, e_x_mask.shape, d_x_mask.shape)\n",
    "    if i > 5: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    w_en = num of encoder vocabulary words\\n    w_de = num of decoder vocabulary words\\n    d / d_model = size of embeddings and num neurons\\n    d_output = output size of encoder/decoder/other module\\n    N = num of transformer blocks (multi-head att + res-block + ff + res-block)\\n    num_heads = num heads in multi-head att layer\\n    drop = num of neurons to drop (dropout)\\n    s = sentence length (num of words in sentence) (is padded to match other items in same batch)\\n    b = batch size\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    w_en = num of encoder vocabulary words\n",
    "    w_de = num of decoder vocabulary words\n",
    "    d / d_model = size of embeddings and num neurons\n",
    "    d_output = output size of encoder/decoder/other module\n",
    "    N = num of transformer blocks (multi-head att + res-block + ff + res-block)\n",
    "    num_heads = num heads in multi-head att layer\n",
    "    drop = num of neurons to drop (dropout)\n",
    "    s = sentence length (num of words in sentence) (is padded to match other items in same batch)\n",
    "    b = batch size\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, num_total_words, d_model):\n",
    "        super().__init__()\n",
    "        self.e = nn.Embedding(num_total_words, embedding_dim=d_model)\n",
    "        self.d = d_model\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ''' [b,s] -> [b,s,d], b=batch size, s=sentence len, d=embedding dim'''\n",
    "        return self.e(x) + np.sqrt(self.d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positional_embedding_matrix(max_seq_len, d):\n",
    "    # returns pe matrix w/ shape = [max_seq_len, d] (d=embed size),\n",
    "    # which encodes (position of word and embedding_number) -> new number\n",
    "    # in practice you do for each embedding pe[:seq_len,:]\n",
    "    pe = torch.zeros([max_seq_len, d]) # positional embedding matrix\n",
    "    for pos in range(max_seq_len):\n",
    "        for i in range(0, d, 2):\n",
    "            pe[pos, i  ] = np.sin( pos / 10000**(2*i/d) )\n",
    "            pe[pos, i+1] = np.cos( pos / 10000**(2*i/d) )\n",
    "    return pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    ''' embeds embeddings and their position to new embdedding '''\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.d = d_model\n",
    "        self.pe = get_positional_embedding_matrix(d=d_model, max_seq_len=1000) # [max_s,d]  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        ''' [b,s,d] -> [1,s,d] '''\n",
    "        return Variable(self.pe[:x.size(1),:x.size(2)], requires_grad=False).unsqueeze(0).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 36]) torch.Size([10, 36, 512]) torch.Size([1, 36, 512])\n",
      "torch.Size([10, 14]) torch.Size([10, 14, 512]) torch.Size([1, 14, 512])\n",
      "torch.Size([10, 19]) torch.Size([10, 19, 512]) torch.Size([1, 19, 512])\n",
      "torch.Size([10, 34]) torch.Size([10, 34, 512]) torch.Size([1, 34, 512])\n",
      "torch.Size([10, 66]) torch.Size([10, 66, 512]) torch.Size([1, 66, 512])\n"
     ]
    }
   ],
   "source": [
    "e = Embedding(num_total_words=100000, d_model=512).cuda()\n",
    "p = PositionalEmbedding(d_model=512)\n",
    "for i, batch in enumerate(iterator):\n",
    "    x = batch.x.transpose(0,1)\n",
    "    z = e(x)\n",
    "    y = p(z)\n",
    "    print(x.shape, z.shape, y.shape)\n",
    "    if i > 3: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = torch.nn.functional.log_softmax\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, d_model, num_total_words):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(d_model, de_total_words) # output/last linear layer\n",
    "    def forward(self, x):\n",
    "        return softmax( self.W(x), dim=-1 )\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, en_total_words, de_total_words, d_model, N, num_heads, drop):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(en_total_words, d_model, N, num_heads, drop)\n",
    "        self.decoder = Decoder(de_total_words, d_model, N, num_heads, drop)\n",
    "        self.generator = Generator(d_model, de_total_words) # output/last linear layer\n",
    "    \n",
    "    def forward(self, e_x, d_x, e_x_mask, d_x_mask):\n",
    "        # e_x  , d_x   = [b, s_e]   , [b, s_d], where s_e = num of words in encoder sentence\n",
    "        # e_x_m, d_x_m = [b, 1, s_e], [b, s_d, s_d]\n",
    "        e_y = self.encoder(e_x, e_x_mask)\n",
    "        d_y = self.decoder(e_y, d_x, e_x_mask, d_x_mask)\n",
    "        return self.generator(d_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder + Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat(module, n):\n",
    "    ''' make n copies of module and store it in list '''\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(n)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_total_words, d_model, N, num_heads, drop):\n",
    "        super().__init__()\n",
    "        self.e = Embedding(num_total_words, d_model)\n",
    "        self.pe = PositionalEmbedding(d_model)\n",
    "        self.dropout = nn.Dropout(drop)\n",
    "        self.multi_head_attention = repeat( MultiHeadAttention(d_model, num_heads, drop), N )\n",
    "        self.add_and_norm_1 = repeat( AddAndNorm(d_model), N )\n",
    "        self.add_and_norm_2 = repeat( AddAndNorm(d_model), N )\n",
    "        self.ff = repeat( FeedForward(d_model), N )\n",
    "        self.N = N\n",
    "        \n",
    "    def forward(self, e_x, e_x_mask):\n",
    "        ''' all x's are w/ shape [b,s,d] '''\n",
    "        e = self.e(e_x) # [b,s,d]\n",
    "        pe = self.pe(e) # [1,s,d]\n",
    "        x = e + pe # broadcast pe for all batches\n",
    "        x = self.dropout(x)\n",
    "        for i in range(self.N):\n",
    "            x_orig = x\n",
    "            x = self.multi_head_attention[i](Q=x, K=x, V=x, mask=e_x_mask)\n",
    "            x = self.add_and_norm_1[i](x, x_orig)\n",
    "            x_orig = x\n",
    "            x = self.ff[i](x)\n",
    "            x = self.add_and_norm_2[i](x, x_orig)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_total_words, d_model, N, num_heads, drop):\n",
    "        super().__init__()\n",
    "        self.e = Embedding(num_total_words, d_model)\n",
    "        self.pe = PositionalEmbedding(d_model)\n",
    "        self.dropout = nn.Dropout(drop)\n",
    "        self.multi_head_attention_1 = repeat( MultiHeadAttention(d_model, num_heads, drop), N )\n",
    "        self.multi_head_attention_2 = repeat( MultiHeadAttention(d_model, num_heads, drop), N )\n",
    "        self.add_and_norm_1 = repeat( AddAndNorm(d_model), N )\n",
    "        self.add_and_norm_2 = repeat( AddAndNorm(d_model), N )\n",
    "        self.add_and_norm_3 = repeat( AddAndNorm(d_model), N )\n",
    "        self.ff = repeat( FeedForward(d_model), N )\n",
    "        self.N = N\n",
    "        \n",
    "    def forward(self, e_y, d_x, e_x_mask, d_x_mask): # encoder_x, decoder_x\n",
    "        ''' [b,s], [b,s], [b,1,s], [b,s,s] -> [b,s,d] '''\n",
    "        d_e = self.e(d_x) # [b,s,d] # decoder embedding\n",
    "        d_pe = self.pe(d_e) # [1,s,d] # decoder positional embed\n",
    "        d_x = d_e + d_pe # broadcast pe for all batches\n",
    "        d_x = self.dropout(d_x)\n",
    "        for i in range(self.N):\n",
    "            x_orig = d_x\n",
    "            d_x = self.multi_head_attention_1[i](Q=d_x, K=d_x, V=d_x, mask=d_x_mask)\n",
    "            d_x = self.add_and_norm_1[i](d_x, x_orig)\n",
    "            x_orig = d_x\n",
    "            d_x = self.multi_head_attention_2[i](Q=d_x, K=e_y, V=e_y, mask=e_x_mask)\n",
    "            d_x = self.add_and_norm_2[i](d_x, x_orig)\n",
    "            x_orig = d_x\n",
    "            d_x = self.ff[i](d_x)\n",
    "            d_x = self.add_and_norm_3[i](d_x, x_orig)\n",
    "        return d_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddAndNorm(nn.Module):\n",
    "    def __init__(self, d_model, epsilon=1e-6):\n",
    "        super().__init__()\n",
    "        self.a = nn.Parameter(torch.ones(d_model))\n",
    "        self.b = nn.Parameter(torch.zeros(d_model))\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def forward(self, x, x_orig):\n",
    "        ''' [b,s,d] -> [b,s,d] '''\n",
    "        mean = x.mean(-1, keepdim=True) # [b,s,1]\n",
    "        std = x.std(-1, keepdim=True) + self.epsilon # [b,s,1]\n",
    "        norm = (x - mean)/std # [b,s,d]\n",
    "        x = self.a * norm + self.b\n",
    "        return x + x_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=2048, drop=0.1):\n",
    "        super().__init__()\n",
    "        self.W1 = nn.Linear(d_model, d_ff)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        self.W2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.functional.relu\n",
    "        \n",
    "    def forward(self, x):\n",
    "        W1, W2, drop, relu = self.W1, self.W2, self.drop, self.relu\n",
    "        return W2( drop( relu(W1(x)) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d, num_heads, drop=0):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.drop1 = nn.Dropout(drop)\n",
    "        self.drop2 = nn.Dropout(drop)\n",
    "        self.saved_attention = None # saved attention outputs [b,h,s,s], h=num heads\n",
    "        self.ff = nn.Linear(d,d)\n",
    "        # linear layers for Q, K, D\n",
    "        self.WQ = nn.Linear(d, d)\n",
    "        self.WK = nn.Linear(d, d)\n",
    "        self.WV = nn.Linear(d, d)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        ''' Q = K = V = [b,s,d] shape, returns same, mask = [b,s,s] '''\n",
    "        softmax = nn.functional.softmax\n",
    "        (b,s,d), h = Q.shape, self.num_heads\n",
    "        # split each var to h heads. [b,s,d] -> [b,h,s,d/h], where h = num heads\n",
    "        # all further computation now is split into h subsets automatically\n",
    "        Q = self.WQ(Q).view(b,-1,h,d//h).transpose(1,2)\n",
    "        K = self.WK(K).view(b,-1,h,d//h).transpose(1,2)\n",
    "        V = self.WV(V).view(b,-1,h,d//h).transpose(1,2)\n",
    "        # [b,h,s,s] = [b,h,s,d/h] @ [b,h,d/h,s] / const\n",
    "        a = (Q @ K.transpose(-2,-1)) / np.sqrt(d//h)\n",
    "        # [b,h,s,s] = [b,h,s,s].masked_fill([b,1,s,s])\n",
    "        a = a if mask is None else a.masked_fill(mask.unsqueeze(1) == 0, -1e9)\n",
    "        self.saved_attention = softmax(a, dim=-1) # TRY: dim=-2\n",
    "        a = self.drop1(self.saved_attention)\n",
    "        # [b,h,s,d/h] = [b,h,s,s] @ [b,h,s,d/h]\n",
    "        a = a @ V\n",
    "        # concat all heads and apply linear layer\n",
    "        a = a.transpose(1,2).contiguous().view(b,s,d)\n",
    "        return self.drop2( self.ff(a) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49739 88914\n",
      "torch.Size([10, 22]) torch.Size([10, 21]) torch.Size([10, 1, 22]) torch.Size([10, 21, 21]) torch.Size([10, 21, 88914])\n",
      "torch.Size([10, 16]) torch.Size([10, 17]) torch.Size([10, 1, 16]) torch.Size([10, 17, 17]) torch.Size([10, 17, 88914])\n",
      "torch.Size([10, 33]) torch.Size([10, 39]) torch.Size([10, 1, 33]) torch.Size([10, 39, 39]) torch.Size([10, 39, 88914])\n"
     ]
    }
   ],
   "source": [
    "en_total_words, de_total_words = len(en_field.vocab.itos), len(de_field.vocab.itos)\n",
    "print(en_total_words, de_total_words)\n",
    "model = Transformer(en_total_words, de_total_words, d_model=512, N=6, num_heads=8, drop=0.1)\n",
    "model = model.cuda()\n",
    "\n",
    "for i, batch in enumerate(iterator):\n",
    "    x = batch.x.transpose(0,1)\n",
    "    y = batch.y.transpose(0,1)\n",
    "    e_x = x\n",
    "    d_x, d_y = y[:,:-1], y[:,1:].contiguous().view(-1)\n",
    "    e_x_mask, d_x_mask = create_enc_dec_masks(e_x,d_x)\n",
    "    z = model(e_x, d_x, e_x_mask, d_x_mask)\n",
    "    # [b,e_s], [b,d_s], [b,1,e_s], [b,d_s,d_s], [b,d_s,total_words], \n",
    "    # where e_s = num of words in encoder sentence, b = batch size\n",
    "    print(e_x.shape, d_x.shape, e_x_mask.shape, d_x_mask.shape, z.shape)\n",
    "    if i > 1: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(en_total_words, de_total_words, d_model=512, N=6, num_heads=8, drop=0.1)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in model.parameters():\n",
    "    if p.dim() > 1: # filter to only ff layers\n",
    "        nn.init.xavier_uniform_(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 15.100875\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 132.38 MiB (GPU 0; 6.00 GiB total capacity; 4.45 GiB already allocated; 69.39 MiB free; 212.02 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-0552244abd62>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;31m# if padding is not ignred, then the loss function will be way lower and will decrease not so fast\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;31m# loss2 = cross_entropy(d_y_pred, d_y)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \"\"\"\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 132.38 MiB (GPU 0; 6.00 GiB total capacity; 4.45 GiB already allocated; 69.39 MiB free; 212.02 MiB cached)"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1\n",
    "cross_entropy = torch.nn.functional.cross_entropy # input:[b,c], target:[b], output:[]\n",
    "model.train() # tell params who needs it to update during training\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    for i, b in enumerate(iterator):\n",
    "        # convert x's y's to correct format for decoder/encoder (e_x, d_x)\n",
    "        x = batch.x.transpose(0,1)\n",
    "        y = batch.y.transpose(0,1)\n",
    "        e_x = x\n",
    "        d_x, d_y = y[:,:-1], y[:,1:].contiguous().view(-1) # [b,d_s], [b x d_s]\n",
    "        e_x_mask, d_x_mask = create_enc_dec_masks(e_x,d_x)\n",
    "        d_y_pred = model(e_x, d_x, e_x_mask, d_x_mask) # [b,d_s,c]\n",
    "        d_y_pred = d_y_pred.view(-1, d_y_pred.size(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss = cross_entropy(d_y_pred, d_y, ignore_index=de_pad_nr) # need to ignore padding\n",
    "        # if padding is not ignred, then the loss function will be way lower and will decrease not so fast\n",
    "        # loss2 = cross_entropy(d_y_pred, d_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 100 == 0: print(i, loss.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_sentence = ['i','love','dogs']\n",
    "max_pred_len = 100\n",
    "e_x = en_field.process([en_sentence]).view(1,len(en_sentence)) # [b,s]\n",
    "for w in e_x[0]:\n",
    "    print(en_field.vocab.itos[w], end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsequent_mask) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, batch in enumerate(iterator):\n",
    "#     x = batch.x.transpose(0,1)\n",
    "#     y = batch.y.transpose(0,1)\n",
    "#     e_x = x[0].unsqueeze(0)\n",
    "#     break\n",
    "# en_sentence = [0] * e_x.size(1)\n",
    "# for d in y[0][1:]: # [b,s], b=1 [1:] to skip <sos> token\n",
    "#     print(de_field.vocab.itos[d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() # turn off dropout\n",
    "e_x_mask = Variable( torch.ones(1,1,len(en_sentence)) ) # [b,1,s]\n",
    "e_x, e_x_mask = e_x.cuda(), e_x_mask.cuda()\n",
    "e_y = model.encoder(e_x, e_x_mask) # [b,s,d] or [1,3,512]\n",
    "d_x = torch.ones(1,1).fill_(de_field.vocab.stoi['<sos>']).type_as(e_x.data) # [b,s]\n",
    "for i in range(max_pred_len):\n",
    "    d_x_mask = Variable(subsequent_mask(d_x.size(1))).type_as(e_x.data) # [b,s,s]\n",
    "    d_y = model.decoder(e_y, d_x, e_x_mask, d_x_mask)\n",
    "    prob = model.generator(d_y) # [b,s,w]\n",
    "#     print(prob.shape)\n",
    "    j, next_word = torch.max(prob, dim=2) # [b,s]\n",
    "    word = torch.full([1, 1], next_word[0,-1]).type_as(e_x.data)\n",
    "    d_x = torch.cat([d_x, word], dim=1) # [b,s] append new word to decoder input\n",
    "    \n",
    "for d in d_x[0][1:]: # [b,s], b=1 [1:] to skip <sos> token\n",
    "    print(de_field.vocab.itos[d])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "11.546175 11.546175\n",
    "10.702625 10.702625\n",
    "10.049401 10.049401\n",
    "9.308085 9.308085\n",
    "8.485468 8.485468\n",
    "7.564091 7.564091\n",
    "6.7078533 6.7078533\n",
    "6.092827 6.092827\n",
    "5.68647 5.68647\n",
    "5.336512 5.336512\n",
    "5.088187 5.088187\n",
    "4.981215 4.981215\n",
    "4.9514318 4.9514318\n",
    "4.95398 4.95398\n",
    "4.928978 4.928978\n",
    "4.9025507 4.9025507\n",
    "4.8787074 4.8787074"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
